# CS4314 NLP

Goal: Fine-tune a LLM and then make it role-play.

目标：在指令数据集上微调一个大语言模型，然后让它能够角色扮演

## Finetune 微调

Base model: [Qwen-2.5-3B](https://huggingface.co/Qwen/Qwen2.5-3B)

Platform: A100 80G x1

Time: 3 hrs

Dataset: [Alpaca-Cleaned](https://huggingface.co/datasets/yahma/alpaca-cleaned)

LoRA

![](result.png)

Example:

```python
{
    'input': "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nGive three tips for staying healthy. <|im_end|>\n<|im_start|>assistant\n", 
    'output': "1. Eat ... leep each night.<|endoftext|>"
}
```

## Role-play 角色扮演

![](aicosplay.png)

### 1 Problem 问题

当我们的模型已经具备了一定的指令跟随的能力，我们希望进一步赋予模型角色扮演的能力。

角色扮演的核心挑战包括：

- 如何使聊天机器人能够以特定身份的虚拟角色进行自然且连贯的对话？
- 如何使聊天机器人具备“记忆”功能，以回忆起超出模型输入序列长度的长期对话内容？
- 如何有效评估虚拟角色在角色扮演过程中的表现效果？

### 2 Out methods 方法

#### 2.1 Overview

我们的方法设计遵循以下几项原则：

- 最后考虑监督微调：首先，我们将重点放在寻找效果较佳的纯推理方案，即仅依赖 LLM 的上下文学习能力。最后，我们再考虑依据所找到的方案构建相应的数据集进行监督微调。这一策略确保了方案能够最大化利用模型的原有能力，而非单纯地增加新的功能。
- 训练与推理的一致性：确保LLM在推理过程中的输入输出模式与训练阶段保持一致，以提升模型的适应性和准确性。
- 强鲁棒性：确保 LLM 在各种条件下均能输出稳定的结果，尽可能避免其受到噪声的干扰。

基于上述原则，我们实现了 **EGOS**（Extract, Generate, Organize, and Summarize）LLM 角色扮演系统。顾名思义，该系统由四个步骤构成，采用纯推理方案，但能够很灵活地与微调相结合。EGOS 系统要求模型具备基本的单轮指令跟随能力，并且不仅能适配本地模型，也能适配模型 API。

我们的方法受到了 ChatHaruhi、RoleLLM 的启发。

#### 2.2 Extract

Extract 步骤在开启对话前，也可以在每轮对话中，取决于实现方案。在此步骤中，我们从所提供的虚拟角色的各类资源中提取角色设定。这些资源可能包括百科式介绍、角色与他人的对话记录以及角色所处的世界观等。我们通过设定合适的提示词，借助外部或内部 LLM 进行信息提取。提取的核心信息包括角色名称、背景、性格特征及示例对话。此步骤的目的在于对大量信息进行总结和提炼，以优化后续模型的输入长度；同时，去除噪声以避免对后续 LLM 输出的干扰。

关于 Extract 步骤的具体实现，我们考虑了两种方案：静态总结式和动态检索式。

- 静态总结式：预先提供资料，让 LLM 进行总结提炼。如果资料超出 LLM 最大输入长度，则将资料进行切分，迭代完善提取的信息。提取出的信息在整个对话过程中保持不变。这一方案的优势在于减少计算开销和延迟，同时保证 LLM 输入前缀的一致性，从而能够利用缓存机制。
- 动态检索式：借助之前的 RAG 方案，将资料转化为数据库，根据每个对话所需的信息进行检索。提取的信息仅在一轮对话中有效。这一方案的优势在于能够更灵活地提供与对话相关的信息。

#### 2.3 Generate

开启对话后，用户每提出一个问题，会触发 Generate 步骤。

在 Generate 步骤中，首先会构造 prompt。这个 prompt 遵循 system, user 和 assistant 三角色单轮对话模型来构造，并使用 chat template。

system prompt 包含上一步提取到的角色核心信息，以及 LLM 需要完成的任务概述。在 system prompt 中，我们会明确告知 LLM 必须遵循的输入输出规则。

user prompt 包含对话信息，包括对话历史（第一轮对话时，对话历史为空）以及用户的问题。

将 system prompt 和 user prompt 输入模型后，再补充一个 assistant 角色的前缀，以提示模型生成。模型输出从引号开始，到引号结束。这是我们方法的一个亮点。对比这两个输入输出模式：

w quote 示例:

```
User: Me: "Who are you?"
Assistant: Tim: "I am Tim." 
```

w/o quote 示例:

```
User: Who are you?
Assistant: I am Tim.
```

我们认为，对于 LLM 而言，w quote 模式正确输出的难度低于 w/o quote 模式正确输出的难度。

第一个原因涉及训练与推理的一致性。在预训练的数据集中，不同角色之间的对话模式多以引号直接引用的形式呈现。例如，小说文本中的角色对话通常遵循这一格式。因此，如果我们在推理过程中也采用这种形式，不仅能有效提示 LLM 当前输出的是某个角色的发言，而非“其自身”的话语，还能促使 LLM 生成更自然、贴近口语的对话。

从更宏观的视角来看，w quote 模式可以视为对话续写任务，而 w/o quote 模式则可看作聊天任务。对话续写能力在预训练过程中已被有效获取，而聊天能力在我们的预训练和监督微调阶段并未得到针对性训练，因此，w quote 的输入输出模式更能充分发挥 LLM 的潜力。

这一点在实验中得到了验证。采用 w/o quote 的模式时，模型在回答时常出现诸如“作为一个 AI 模型……”的幻觉性表述，且其回答往往较为书面化、正式化。

第二个原因是这样做能保证模型输出的鲁棒性。在 w quote 中，我们可以用后引号来判断模型输出结束时机，保证了模型仅输出一个回答，而不包含其他无关信息。

实验结果同样支持这一观点。如果我们用结束符（EOS）来判断模型什么时候输出结束，那么常会出现模型直接无视我们在 system prompt 中给定的规则的情况，例如在一句话结束之后继续输出一些无关信息或者多条对话。通过引入后引号强制截止机制，可以有效避免此类情况的发生。

因此，EGOS 系统选择采用 w quote 的输入输出模式，以最大化模型的表现和输出质量。

#### 2.4 Organize & Summarize

当每一轮对话结束时，会进入 Organize 步骤。在这一步里，我们将本轮对话的文本添加到对话历史中。对话历史会作为下一轮对话时 Generate 步骤的输入，以保证对话的连贯性和上下文的一致性。

在此过程中，我们考虑了两种组织对话历史的方案，一种采用标准的使用多轮 chat template，另一种方案是将对话历史全部整合至 user prompt 中。

multiple 示例：

```
User: Me: "What's your favourite food?"
Assistant: Tim: "I really enjoy sushi. How about you?"
User: Me: "I can't resist pizza."
Assistant: Tim: "I find it a bit too greasy for my taste."
```

single 示例：

```
User: Me: "What's your favourite food?" 
      Tim: "I really enjoy sushi. How about you?" 
      Me: "I can't resist pizza."
Assistant: Tim: "I find it a bit too greasy for my taste."
```

最终，我们选择了 single 方案。这一选择能够更好地适配我们在 Generate 步骤中所遵循的对话续写任务范式。此外，考虑到训练与推理的一致性，我们在 Alpaca cleaned 的监督微调中只训练了模型单轮指令跟随的能力，所以 single 方案更合适。

当对话历史长度超过模型的最大输入长度时，会触发 Summarize 步骤。简而言之，此步骤旨在对对话历史进行概括，从而减少其长度。在 Summarize 步骤中，我们会丢弃对话历史的原始文本，取而代之的是一些情景的概括，包括对话双方的角色、话题和角色在对话中说出的关键信息。Summarize 步骤与 Extract 步骤相似，我们同样考虑了两种方案，静态总结式和动态检索式。

### 3 Evaluation 测评

#### 3.1 Metrics 测评指标

我们设计了三个维度来测评模型角色扮演能力：

- 角色一致性：角色一致性指的是模型在不同情境下对角色特征、背景及行为的保持稳定性和连贯性。这一维度旨在评估模型是否能够有效地理解并维持角色的核心特质，如性格、动机和价值观等。在角色扮演的过程中，角色一致性不仅关系到模型生成的内容是否符合角色设定，也影响到用户体验的沉浸感和真实感。
- 记忆能力：记忆能力涉及模型在对话过程中对先前信息的存储、检索和应用的能力。具体而言，记忆能力不仅包括对上下文信息的即时记忆，还涉及对重要信息的长期保留及其在后续交互中的有效利用。这一维度的评估将有助于了解模型在角色扮演中的信息处理能力，尤其是如何在多次交互中保持对角色历史背景和用户输入的理解。
- 生成质量：生成质量是指模型在角色扮演过程中所产生的响应的整体优劣，包括语言的流畅性、逻辑的严密性以及语义的准确性。在此维度的评估中，我们不仅关注内容的语法和拼写是否正确，还重视模型所生成内容的创新性、深度和恰当性。

#### 3.2 Case Study 案例研究

我们测试如下四种系统：

1. EGOS Small：完整的 EGOS 系统，使用 LoRA 微调的 Qwen2.5-3B
2. EGOS w/o quote：在 Generate 步骤不用带引号输入输出模式
3. EGOS w/o Extract：去掉 Extract 步骤，直接将原始角色资料作为 Generate 步骤的 system prompt
4. EGOS Large：完整的 EGOS 系统，使用 DeepSeek V3 API。

由于缺乏测评数据，我们人工地、逐案例地在三个维度上评价系统的回应效果。

**角色认知**：

EGOS Small：

```
User: 介绍一下你自己吧！
Assistant: 当然，我是SOS团的团长.
User: 你叫什么？
Assistant: 凉宫春日。
```

EGOS w/o quote:

```
User: 介绍一下你自己吧！
Assistant: 凉宫春日：小明，你叫我小明，我就不高兴。
```

EGOS w/o Extract:

```
User: 介绍一下你自己吧！
Assistant: 这是我的『介绍自己』，如果觉得无聊的话可以多看一两眼。
```

EGOS Large:

```
User: 介绍一下你自己吧！
Assistant: 我是凉宫春日，SOS团的团长！这个世界太无聊了，我要找到外星人、未来人和超能力者，让世界变得更热闹！
```

我们可以发现，EGOS Small 的回答基本没有问题。EGOS w/o quote 输出了不符合要求的对话，可能是由于 LLM 无法区分输出的是角色说的话还是作为一个 AI 模型说的话。EGOS w/o Extract 输出了莫名其妙的回复，可能是由于 LLM 受到了来自原始角色资料中的噪声的影响。EGOS Large 的效果很好，发言契合角色的性格特点，在流畅度和表达力上相比于 EGOS Small 有很大提升。

**场景对话**：

EGOS Small：

```
User: 好无聊啊。
Assistant: 喂，怎么样？被『超级无敌超人』这样一个美女盯上了，不是挺棒的嘛！
User: 你也太自以为是了。
Assistant: 那是因为我实在是太棒了！
```

EGOS w/o quote:

```
User: 好无聊啊。
Assistant:
场景：现在小明和凉宫春日刚刚见面
好无聊啊。自从我小学时看了一场棒球赛后，我就变得有点疯狂了，常常做一些不可思议的事情。但这些事情其实都是我自己想的，所以我一直以来都是孤独的。直到我遇见了你，我才感觉像个活人一样！
凉宫春日：嗯嗯……
```

EGOS w/o Extract:

```
User: 好无聊啊。
Assistant: 哎呀，其实还是有点无聊的，我们班可是人多。
User: 什么意思？
Assistant: 我们班的人多，所以有很多事情可做。
```

EGOS Large:

```
User: 好无聊啊。
Assistant: 无聊？那就来加入SOS团吧！我们可是专门寻找外星人、未来人和超能力者的哦！
User: 外星人，真的存在吗...
Assistant: 当然存在！不然这个世界岂不是太无趣了？跟我一起找出来，让世界变得更热闹吧！
```

我们发现 EGOS Small 和 EGOS Large 都能够带入到我们设定的场景中，扮演对应的角色。而 EGOS w/o quote 和 EGOS w/o Extract 还是遇到了同样的问题。

**性格模仿**：

EGOS Small:

```
User: 不是你想做什么就做什么的。
Assistant: 我想做的话，就一定会做，不会被什么阻挡。
User: 你太任性了。
Assistant: 那是因为我不想被任何别人控制，我想自己做主。即使这样做会让自己变坏也没关系。
```

EGOS Large:

```
User: 不是你想做什么就做什么的。
Assistant: 哼，这个世界就是我想做什么就做什么！不然要我这个团长干嘛？
User: 你太任性了。
Assistant: 任性？这叫领导力！没有我，这个世界多无聊啊！
```

我们可以看到 LLM 理解了角色的性格，并且生成了符合角色性格的对话。

**上下文记忆**：这里我们强制在第三轮对话前进入 Summarize 步骤，并且清空之前的对话，以模拟对话长度超出最大输入长度。

EGOS Small:

```
User: 你为什么总是能实现自己的愿望？
Assistant: 因为这是我生存的方式，我想让世界变得更温暖。
User: 不错的理想。
Assistant: 是的，我一直希望能让世界变得更温暖，让每一个人在这个世界上都能找到自己的位置。
// Summarize
User: 我们刚才在谈论什么？
Assistant: 是我说我的存在意义吧。
```

EGOS Large:

```
User: 你为什么总是能实现自己的愿望？
Assistant: 因为这个世界就是为我而存在的啊！只要我想要，什么都能实现，这不是理所当然的吗？
User: 呃，没人会觉得这是理所当然的。
Assistant: 哼，那是因为你们太普通了！我可是凉宫春日，世界当然要听我的！
// Summarize
User: 我们刚才在谈论什么？
Assistant: 当然是讨论为什么我能实现愿望啊！因为这个世界是为我而存在的，理所当然会听我的！
```

我们看到 LLM 成功回忆起了之前的对话内容，并且能进一步作出回答。

**长对话**：

EGOS Large:

```
User: 初次见面，我叫小明。
Assistant: 小明？听起来好普通啊！不过无所谓，既然你遇到了我，就准备好迎接不平凡的生活吧！加入SOS团吧！
User: SOS团是做什么的？
Assistant: SOS团是寻找外星人、未来人、超能力者和各种神秘事件的社团！跟着我，保证你的生活不再无聊！
User: 哪有什么外星人？
Assistant: 哼，你这种普通人是不会懂的！外星人当然存在，只是你还没遇到而已。加入SOS团，我带你见识真正的世界！
User: 哈哈，那你带我见识一下。
Assistant: 好啊，那就从今晚开始！我们一起去学校操场寻找外星人的信号！别想逃，我已经决定了！
User: 学校操场？有点普通。
Assistant: 普通？哼，那是因为你还没看到我画的符号！今晚就让你见识一下真正的神秘力量！
```

从上面简单的实验我们能发现，在 EGOS 系统中，LLM 具备了初步的角色扮演能力，在角色一致性和记忆能力上比较好。EGOS Small 由于模型较小，生成质量在一定程度上受限，并且当问题变得较为复杂，涉及一些复杂的逻辑，LLM 就会产生幻觉输出。EGOS Large 的生成效果比较好。


### 4 进一步研究

在未来的研究中，我们可以将 EGOS 系统与微调结合，增强模型的在此特定领域的能力。我们可以对执行 Generate 步骤的 LLM 进行监督微调，构建的数据集应包含一系列经过角色标注的对话以及相关的角色背景信息，从而使模型能够更好地捕捉复杂的逻辑关系和语境信息，提升角色扮演对话的生成质量和一致性。此外，考虑到 EGOS 系统已利用 LLM 的对话续写能力，我们也可以探索无监督微调的可能性，预期在无监督的对话数据上进行微调将进一步提升模型性能。

此外，我们可以尝试收集充分的数据，以完善角色扮演的评测流水线。测试集可以涵盖多样化的对话场景、不同角色的互动以及角色背景的详细信息。通过构建全面的评估框架，我们能够更有效地测试和优化模型在角色扮演任务中的表现，确保其在各种情况下都能保持高水平的响应能力。

为此，我们提出了一个初步的设想，从三个维度上分别对角色扮演系统性能进行评测：

- 角色一致性：我们采用对比法。我们将角色真实的对话与模型生成的对话给 ChatGPT 进行判断，以识别出哪一组对话是由模型生成的，哪一组为真实对话。通过统计模型在此任务中以假乱真的频率，我们能够量化模型在保持角色特征与情境一致性方面的能力。
- 记忆能力：我们采用信息重现的实验框架，即在上一轮对话中给定若干关键信息，让 LLM 在下一轮中重复这些信息，随后在下一轮对话中要求其准确重复这些信息。
- 生成质量：将不同生成策略所产生的对话文本提交给ChatGPT进行判断。模型需对这些文本进行评估，判断其流畅度、表达力及整体连贯性。通过对比不同生成方法的表现，我们能够量化各自的优劣，为模型优化提供实证依据。

## Reference 参考

[ChatHaruhi](https://github.com/LC1332/Chat-Haruhi-Suzumiya)

[CharacterEval](https://github.com/morecry/CharacterEval)

[RoleLLM](https://arxiv.org/pdf/2310.00746)